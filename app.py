import streamlit as st
import os
import tempfile
import io
import re

# --- 1. å¯¼å…¥åŸºç¡€ä¾èµ– ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_core.documents import Document  # ç”¨äºæ„å»º PPT æ–‡æ¡£å¯¹è±¡

# --- 2. å¯¼å…¥æ–°åŠŸèƒ½ä¾èµ– (PPT å’Œ Word) ---
from pptx import Presentation
from docx import Document as DocxDocument
from docx.shared import Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
# âœ¨ å…³é”®ä¿®å¤ï¼šå¯¼å…¥ XML å‘½åç©ºé—´å¤„ç†ä¸­æ–‡å­—ä½“
from docx.oxml.ns import qn

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="DeepSeek å…¨èƒ½ç ”æŠ¥åŠ©æ‰‹ (ä¿®å¤ä¹±ç ç‰ˆ)", layout="wide", page_icon="ğŸ“")

st.title("ğŸ“ DeepSeek å…¨èƒ½ç ”æŠ¥ç”Ÿæˆå™¨")
st.markdown("æ”¯æŒ **PDF & PPT** æ··åˆä¸Šä¼  | ç”Ÿæˆ **Word (.docx)** æŠ¥å‘Š (å·²ä¿®å¤ä¸­æ–‡ä¹±ç )")
st.markdown("---")

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    api_key = st.text_input("è¯·è¾“å…¥ DeepSeek API Key", type="password")
    st.markdown("[ğŸ‘‰ ç‚¹å‡»è¿™é‡Œç”³è¯· DeepSeek Key](https://platform.deepseek.com/)")
    st.markdown("---")
    st.info("ğŸ’¡ **å‡çº§è¯´æ˜**ï¼š\n1. ä¿®å¤äº†å¯¼å‡º Word æ—¶ä¸­æ–‡æ˜¾ç¤ºä¸ºæ–¹æ¡†çš„é—®é¢˜ã€‚\n2. é»˜è®¤ä½¿ç”¨ **å¾®è½¯é›…é»‘** å­—ä½“ã€‚")


# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

@st.cache_resource
def get_embedding_model():
    with st.spinner("æ­£åœ¨åŠ è½½æœ¬åœ°å‘é‡æ¨¡å‹..."):
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")


def load_ppt_file(file_path):
    """è§£æ PPT æ–‡ä»¶"""
    prs = Presentation(file_path)
    documents = []

    for i, slide in enumerate(prs.slides):
        slide_text = []
        for shape in slide.shapes:
            if hasattr(shape, "text") and shape.text.strip():
                slide_text.append(shape.text.strip())

        if slide_text:
            content = "\n".join(slide_text)
            metadata = {"page": i + 1}
            documents.append(Document(page_content=content, metadata=metadata))

    return documents


def process_files(uploaded_files, embeddings):
    """å¤„ç†æ··åˆæ–‡ä»¶ (PDF + PPT)"""
    if not uploaded_files:
        return None

    all_documents = []
    progress_text = "æ­£åœ¨è§£ææ–‡æ¡£..."
    my_bar = st.progress(0, text=progress_text)
    total_files = len(uploaded_files)

    for i, uploaded_file in enumerate(uploaded_files):
        my_bar.progress((i / total_files), text=f"æ­£åœ¨è§£æ: {uploaded_file.name}")

        file_ext = os.path.splitext(uploaded_file.name)[1].lower()

        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        try:
            docs = []
            if file_ext == ".pdf":
                loader = PyPDFLoader(tmp_file_path)
                docs = loader.load()
            elif file_ext in [".ppt", ".pptx"]:
                docs = load_ppt_file(tmp_file_path)
            else:
                st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {uploaded_file.name}")
                continue

            for doc in docs:
                doc.metadata['source_filename'] = uploaded_file.name

            all_documents.extend(docs)

        except Exception as e:
            st.error(f"è§£æ {uploaded_file.name} å¤±è´¥: {e}")
        finally:
            if os.path.exists(tmp_file_path):
                os.remove(tmp_file_path)

    my_bar.progress(1.0, text="æ–‡æ¡£è§£æå®Œæˆï¼Œæ­£åœ¨å»ºç«‹å‘é‡åº“...")

    if not all_documents:
        return None

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150
    )
    texts = text_splitter.split_documents(all_documents)

    db = Chroma.from_documents(texts, embeddings)
    my_bar.empty()
    return db


def generate_report(db, topic, api_key):
    """è°ƒç”¨ DeepSeek ç”Ÿæˆå†…å®¹"""
    prompt_template = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é«˜çº§å•†ä¸šåˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹ã€å¤šä»½æ–‡æ¡£å†…å®¹ã€‘æ’°å†™ä¸€ä»½è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚

    ã€ç»¼åˆå‚è€ƒä¿¡æ¯ã€‘:
    {context}

    ã€ç”¨æˆ·æŒ‡ä»¤ã€‘: 
    {question}

    ã€æ’°å†™è¦æ±‚ã€‘:
    1. **æ ¼å¼**: å¿…é¡»ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆä½¿ç”¨ # è¡¨ç¤ºä¸€çº§æ ‡é¢˜ï¼Œ## è¡¨ç¤ºäºŒçº§æ ‡é¢˜ï¼Œ- è¡¨ç¤ºåˆ—è¡¨ï¼‰ã€‚
    2. **å†…å®¹**: æ·±åº¦æ•´åˆä¸åŒæ–‡æ¡£çš„æ•°æ®ã€‚
    3. **å¼•ç”¨**: åœ¨å…³é”®æ•°æ®åæ ‡æ³¨æ¥æºã€‚
    4. **ä¸¥è°¨**: ä»…åŸºäºç»™å®šææ–™ï¼Œä¸ç¼–é€ ã€‚

    è¯·å¼€å§‹æ’°å†™:
    """

    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    llm = ChatOpenAI(
        model_name="deepseek-chat",
        openai_api_key=api_key,
        openai_api_base="https://api.deepseek.com",
        temperature=0,
        max_tokens=3000
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 8}),
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    return qa_chain.invoke(topic)


def create_word_docx(markdown_text):
    """
    âœ¨ ä¿®å¤ç‰ˆï¼šç”Ÿæˆæ”¯æŒä¸­æ–‡çš„ Word æ–‡æ¡£
    """
    doc = DocxDocument()

    # --- å…³é”®ä¿®å¤ï¼šè®¾ç½®å…¨å±€ä¸­æ–‡å­—ä½“ ---
    style = doc.styles['Normal']
    style.font.name = 'Microsoft YaHei'  # è®¾ç½®è¥¿æ–‡å­—ä½“
    style.font.element.rPr.rFonts.set(qn('w:eastAsia'), 'Microsoft YaHei')  # è®¾ç½®ä¸­æ–‡å­—ä½“ (å¾®è½¯é›…é»‘)
    style.font.size = Pt(11)

    lines = markdown_text.split('\n')

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # ç®€å•è§£æ Markdown æ ‡é¢˜
        if line.startswith('# '):
            heading = doc.add_heading(line.replace('# ', ''), level=1)
            # ä¸ºæ ‡é¢˜ä¹Ÿè®¾ç½®å­—ä½“ï¼ˆé˜²æ­¢æ ‡é¢˜ä¹±ç ï¼‰
            for run in heading.runs:
                run.font.name = 'Microsoft YaHei'
                run.font.element.rPr.rFonts.set(qn('w:eastAsia'), 'Microsoft YaHei')

        elif line.startswith('## '):
            heading = doc.add_heading(line.replace('## ', ''), level=2)
            for run in heading.runs:
                run.font.name = 'Microsoft YaHei'
                run.font.element.rPr.rFonts.set(qn('w:eastAsia'), 'Microsoft YaHei')

        elif line.startswith('### '):
            heading = doc.add_heading(line.replace('### ', ''), level=3)
            for run in heading.runs:
                run.font.name = 'Microsoft YaHei'
                run.font.element.rPr.rFonts.set(qn('w:eastAsia'), 'Microsoft YaHei')

        elif line.startswith('- ') or line.startswith('* '):
            p = doc.add_paragraph(line.replace('- ', '').replace('* ', ''), style='List Bullet')
        else:
            doc.add_paragraph(line)

    # ä¿å­˜åˆ°å†…å­˜æµ
    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio


# --- ä¸»ç•Œé¢é€»è¾‘ ---

if not api_key:
    st.warning("âš ï¸ è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ è¾“å…¥ DeepSeek API Keyã€‚")
else:
    embedding_model = get_embedding_model()

    uploaded_files = st.file_uploader(
        "ğŸ“„ ä¸Šä¼ æ–‡æ¡£ (æ”¯æŒ PDF å’Œ PPTX)",
        type=["pdf", "pptx", "ppt"],
        accept_multiple_files=True
    )

    if uploaded_files:
        current_file_names = [f.name for f in uploaded_files]

        if "last_uploaded_files_mix" not in st.session_state or st.session_state.last_uploaded_files_mix != current_file_names:
            st.session_state.vector_db_mix = process_files(uploaded_files, embedding_model)
            st.session_state.last_uploaded_files_mix = current_file_names
            if st.session_state.vector_db_mix:
                st.success(f"âœ… å·²æˆåŠŸè§£æ {len(uploaded_files)} ä»½æ–‡æ¡£ï¼")

        st.subheader("ğŸ“Š æŠ¥å‘Šç”Ÿæˆè®¾ç½®")
        default_topic = "ç»¼åˆåˆ†æè¿™äº›æ–‡æ¡£ï¼Œè¾“å‡ºä¸€ä»½åŒ…å«æ‘˜è¦ã€å…³é”®å‘ç°å’Œç»“è®ºçš„å®Œæ•´æŠ¥å‘Šã€‚"
        report_topic = st.text_area("åˆ†ææŒ‡ä»¤:", value=default_topic, height=100)

        if st.button("ğŸš€ ç”Ÿæˆå¹¶å¯¼å‡ºæŠ¥å‘Š"):
            if "vector_db_mix" in st.session_state:
                with st.spinner("æ­£åœ¨æ€è€ƒå¹¶æ’°å†™ Word æŠ¥å‘Š..."):
                    try:
                        response = generate_report(st.session_state.vector_db_mix, report_topic, api_key)
                        report_content = response['result']

                        st.markdown("### ğŸ“„ æŠ¥å‘Šé¢„è§ˆ")
                        st.markdown(report_content)
                        st.markdown("---")

                        # ç”Ÿæˆä¿®å¤ä¹±ç åçš„ Word
                        docx_file = create_word_docx(report_content)

                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½ Word æŠ¥å‘Š (.docx)",
                            data=docx_file,
                            file_name="DeepSeek_åˆ†ææŠ¥å‘Š.docx",
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                        )

                        with st.expander("ğŸ” æŸ¥çœ‹å¼•ç”¨æ¥æº"):
                            for i, doc in enumerate(response['source_documents']):
                                source = doc.metadata.get('source_filename', 'æœªçŸ¥æ–‡ä»¶')
                                page = doc.metadata.get('page', '?')
                                st.markdown(f"**[{i + 1}] {source} (ç¬¬ {page} é¡µ/å¼ ):**")
                                st.caption(f"> {doc.page_content[:150]}...")
                                st.divider()

                    except Exception as e:
                        st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            else:
                st.error("è¯·ç­‰å¾…æ–‡æ¡£è§£æå®Œæˆã€‚")